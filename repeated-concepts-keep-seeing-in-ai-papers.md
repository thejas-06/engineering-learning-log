# Repeated Concepts I Keep Seeing in AI Papers

While reading multiple AI research papers, I noticed the same set of concepts
appearing again and again. 

---

## Edge AI
AI that runs directly on the device instead of the cloud.

Example I should remember:
- Face recognition on a phone working without internet

Why it keeps appearing:
- Low latency
- Privacy
- No cloud dependency

---

## BERT
A language model introduced by Google in 2018.

Key idea:
- Understands *context* by looking at words in both directions

Why it matters:
- Better sentence understanding than older models

---

## LLaMA
An open-weight language model released by Meta.

How to remember it:
- Research-friendly alternative to closed models
- Often compared to ChatGPT in capability, but open

---

## Ollama
Software that lets me run large AI models locally.

Mental shortcut:
- Like installing an app to run LLaMA on my own computer

Why it matters:
- No cloud
- Local experimentation
- Privacy

---

## Neuromorphic Computing
Computing inspired by how human brain neurons work.

Key intuition:
- Event-driven instead of constant computation

Why papers talk about it:
- Extremely power efficient
- Potential for devices that run for years on tiny batteries

---

## State-Based / Streaming Models
Models that process information as a continuous flow instead of fixed chunks.

How to remember:
- Better for long videos, streams, and time-based data

Why it matters:
- Scales to hours of information without breaking context

---

## Quantum Statistical Query Methods
Learning approaches that apply quantum principles.

Key takeaway:
- Certain problems can be solved exponentially faster

Real-world signal:
- Used to compress models drastically while keeping performance

---

## Multimodal AI
AI that understands multiple data types together.

Example:
- Seeing an image, hearing audio, and answering in text

Why it matters:
- More human-like interaction
- Broader real-world applications

---
